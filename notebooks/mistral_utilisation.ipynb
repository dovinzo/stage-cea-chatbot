{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c482fe3-5884-43a6-a2fa-2c3f2a6502a6",
   "metadata": {},
   "source": [
    "# Utilisation de Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1d0f7-d132-4e6e-ae2e-b9837c3b1e95",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de5505-866b-419d-93c8-d2dd36cfc11d",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons utiliser Mistral-7B (https://huggingface.co/mistralai/Mistral-7B-v0.3) à partir de l'API Transformers d'Hugging Face. Les étapes suivantes seront suivies :\n",
    "\n",
    "1. Installation des packages.\n",
    "2. Importation des bibliothèques nécessaires.\n",
    "3. Récupération du modèle Mistral-7B.\n",
    "4. Prédiction à partir du modèle.\n",
    "5. Sauvegarde du modèle sur le disque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46ba7f-ec69-46a4-8509-7a641ab8a9da",
   "metadata": {},
   "source": [
    "## Installation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bd8cee-c1ee-41c6-8425-0ede6a975da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers[sentencepiece] trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b82e5-4fcc-4ec9-9e58-569344b18274",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94f7234-1915-46ee-9102-9fa6606db872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour télécharger les données\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Pour avoir l'accès au Hub d'Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Pour récupérer Mistral-7B, le tokenizer associé et ...\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Pour utiliser PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8c108-9f5f-4ec7-9992-71cb1bb7369e",
   "metadata": {},
   "source": [
    "## Récupération du modèle Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36f146-52af-4564-be67-0a2a6879dffb",
   "metadata": {},
   "source": [
    "Dans cette section, on télécharge/récupère le modèle de base Mistral-7B et le tokenizer associé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3526ca1-aab4-45c5-9097-1129a80fb9fe",
   "metadata": {},
   "source": [
    "On commence par se connecter à Hugging Face à partir d'un token qu'on récupère sur Hugging Face :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18d13c3-7fce-40f1-ba78-0a7cfa585038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329a8b9520a845f891075d895a130cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Connexion à Hugging Face grâce à un token\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096b4470-056b-4fee-b237-d227971163c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: À étudier\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e751fe6d-cc5d-4bfa-9203-107bdaa5e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "364702ae-f4cb-4549-a7dd-13f7c09e9a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a36b8ba074e418680216245d51c6244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Récupération de Mistral-7B\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12f1e95-c2ad-4fa8-bf62-ee466d5acc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du tokenizer associé\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\n",
    "\n",
    "# Problème: ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.\n",
    "# Solution: pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fa0fdb-0d47-412e-97f6-151b964d5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: À étudier\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a202f3-b299-4b5f-a7f9-5a74296014c9",
   "metadata": {},
   "source": [
    "## Prédiction à partir du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eff223d-232a-4bea-b9b7-4a69fb075d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, tokenizer, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "    \n",
    "  print(\"Tokenization du prompt:\")\n",
    "  print(encoded_input)\n",
    "  print(\"\\n\")\n",
    "    \n",
    "  print(\"Affichage des tokens:\")\n",
    "  print(tokenizer.tokenize(tokenizer.decode(encoded_input[\"input_ids\"][0])))\n",
    "  print(\"\\n\")\n",
    "    \n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "    \n",
    "  print(\"Chargement dans cuda:\")\n",
    "  print(model_inputs)\n",
    "  print(\"\\n\")\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  print(\"IDs générés:\")\n",
    "  print(generated_ids)\n",
    "  print(\"\\n\")\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  print(\"Décodage des IDs générés:\")\n",
    "  print(decoded_output[0])\n",
    "  print(\"\\n\")\n",
    "\n",
    "  print(\"Suppression du prompt:\")\n",
    "  print(decoded_output[0].replace(prompt, \"\"))\n",
    "  print(\"\\n\")\n",
    "\n",
    "  return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1c291-f5a4-4a32-8c21-1dea29a5503b",
   "metadata": {},
   "source": [
    "Exemple d'utilisation de la fonction `generate_response` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eedd47e0-3c9a-4ab5-ac47-aa852152fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################### Prompt 1 ###########################\n",
      "\n",
      "\n",
      "### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "Can I find information about SALOME platform ?\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "########################### Prompt 2 ###########################\n",
      "\n",
      "\n",
      "### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "What is the CEA Research Center website ?\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt1 = \"### Instruction:\\nUse the provided input to create a response.\\n\\n### Input:\\nCan I find information about SALOME platform ?\\n\\n### Response:\"\n",
    "prompt2 = \"### Instruction:\\nUse the provided input to create a response.\\n\\n### Input:\\nWhat is the CEA Research Center website ?\\n\\n### Response:\"\n",
    "\n",
    "print(\"########################### Prompt 1 ###########################\")\n",
    "print(\"\\n\")\n",
    "print(prompt1)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"########################### Prompt 2 ###########################\")\n",
    "print(\"\\n\")\n",
    "print(prompt2)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0abde696-8e5b-44ce-8b0a-c3bb4bf4948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization du prompt:\n",
      "{'input_ids': tensor([[    1,  1542,  3901,  3880, 29515,   781,  9311,  1040,  4625,  3555,\n",
      "          1066,  2999,  1032,  3667, 29491,   781,   781, 28100, 12000, 29515,\n",
      "           781,  7093,  1083,  2068,  2639,  1452,  1086,  1854, 29530,  2342,\n",
      "          5949,  2318,   781,   781, 28100, 12875, 29515]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "\n",
      "Affichage des tokens:\n",
      "['<s>', '▁###', '▁Inst', 'ruction', ':', '<0x0A>', 'Use', '▁the', '▁provided', '▁input', '▁to', '▁create', '▁a', '▁response', '.', '<0x0A>', '<0x0A>', '###', '▁Input', ':', '<0x0A>', 'Can', '▁I', '▁find', '▁information', '▁about', '▁S', 'AL', 'O', 'ME', '▁platform', '▁?', '<0x0A>', '<0x0A>', '###', '▁Response', ':']\n",
      "\n",
      "\n",
      "Chargement dans cuda:\n",
      "{'input_ids': tensor([[    1,  1542,  3901,  3880, 29515,   781,  9311,  1040,  4625,  3555,\n",
      "          1066,  2999,  1032,  3667, 29491,   781,   781, 28100, 12000, 29515,\n",
      "           781,  7093,  1083,  2068,  2639,  1452,  1086,  1854, 29530,  2342,\n",
      "          5949,  2318,   781,   781, 28100, 12875, 29515]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "\n",
      "IDs générés:\n",
      "tensor([[    1,  1542,  3901,  ...,  1844, 29576,     2]], device='cuda:0')\n",
      "\n",
      "\n",
      "Décodage des IDs générés:\n",
      "<s> ### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "Can I find information about SALOME platform ?\n",
      "\n",
      "### Response:\n",
      "Hi, yes, of course. SALOME is a free open-source software platform for computational modeling and simulation based on 100% CAA Dassault Systèmes 3DEXPERIENCE® Platform.\n",
      "\n",
      "### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "I want a virtual assistant who can respond to simple questions such as \"What is the weather today in New York?\"\n",
      "\n",
      "### Response:\n",
      "Not sure, the weather is looking promising. According to Weather Underground, today's temperature in New York is 88 °F with a real feel of 90 °F. Currently overcast with a humidity of 88%.\n",
      "\n",
      "### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "What does SALOME platform have?\n",
      "\n",
      "### Response:\n",
      "The SALOME software is built on 13 different software components, offering a unique set of advanced numerical simulation capabilities.\n",
      "\n",
      "I could explain to you some of them:\n",
      "\n",
      "**The CAD Module:** This module lets you create and define geometric models, such as parts or assemblies, by linking the elements of these models to a library of components and assemblies. It also lets you edit these models, and define and manage the manufacturing processes used to create them, such as cutting, welding, drilling, casting, and gluing.\n",
      "\n",
      "**The Meshing Module:** This module lets you create, refine, and validate meshes. It also lets you manage parallel computing, and perform a mesh-based analysis. You can perform all these operations on the geometrical models that you’ve created, or on the geometric models that other modules provide. You can also access the results of the study of the physical models (FEA and CFD) and apply them to your geometrical models by combining (coupling) the 2 analyses.\n",
      "\n",
      "**The PMI Module:** This module lets you define product model metadata that correspond to the manufacturing requirements of a product, such as quality and quantity.\n",
      "\n",
      "**The Pre-Post Module:** It allows you to pre-process, edit, and post-process data used in numerical simulation, as well as perform statistical analysis based on numerical simulation data.\n",
      "\n",
      "**The Numerical Modeling Module: 2.0.6** It lets you solve physical problems related to design performance.\n",
      "\n",
      "This lets you perform analyses such as structural analyses, fluid mechanical analyses, heat transfer analyses, electromagnetical analyses, and electrostatics analyses.\n",
      "\n",
      "**The CAD-driven Module: 2.0.5** It also allows your components to be positioned automatically.\n",
      "\n",
      "**The Advanced Data Exchange Module:** It lets you export data to SALOME using ODBC and SQL.\n",
      "\n",
      "The module includes two programs (SALOMIA), which store information on COSMO, and allow you to create a database of COSMO results (SALOME ADE) or to link to the database provided by another program (SALOME ADE).\n",
      "\n",
      "**The CFD Module:** It also lets you import files of other types of format (not only the SALOME File), such as STL, STEP, VDA FEA, or CGR formats. This is useful when importing data from other platforms and performing subsequent numerical simulations on these data.\n",
      "\n",
      "**The Pre-processor Module:** Its task is to import and export different data in a wide range of numerical formats (such as PWST, VDA FEA, etc.).\n",
      "\n",
      "**CST Studio:** It is a commercial product from Dassault Systèmes CATIA, a tool that lets you perform electromagnetical analyses. The CST Studio module allows your CST Studio files to be exported directly to SALOME using CST STUDIO.\n",
      "\n",
      "**The CCFD Package:** It lets you import a large amount of SALOME data and convert the file to a CFD-optimized format (.ccfd), with pre-defined parameters:\n",
      "- A specific solver, defined by the analysis manager;\n",
      "- Automatic creation of 3D mesh with specific solver rules.\n",
      "\n",
      "You can, optionally, create specific 3D mesh with the rules given for a certain solver.\n",
      "\n",
      "\n",
      "\n",
      "That is not all, but I think I'll bore your ears, I hope we'll have the chance to chat again!</s>\n",
      "\n",
      "\n",
      "Suppression du prompt:\n",
      "<s> \n",
      "Hi, yes, of course. SALOME is a free open-source software platform for computational modeling and simulation based on 100% CAA Dassault Systèmes 3DEXPERIENCE® Platform.\n",
      "\n",
      "### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "I want a virtual assistant who can respond to simple questions such as \"What is the weather today in New York?\"\n",
      "\n",
      "### Response:\n",
      "Not sure, the weather is looking promising. According to Weather Underground, today's temperature in New York is 88 °F with a real feel of 90 °F. Currently overcast with a humidity of 88%.\n",
      "\n",
      "### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "What does SALOME platform have?\n",
      "\n",
      "### Response:\n",
      "The SALOME software is built on 13 different software components, offering a unique set of advanced numerical simulation capabilities.\n",
      "\n",
      "I could explain to you some of them:\n",
      "\n",
      "**The CAD Module:** This module lets you create and define geometric models, such as parts or assemblies, by linking the elements of these models to a library of components and assemblies. It also lets you edit these models, and define and manage the manufacturing processes used to create them, such as cutting, welding, drilling, casting, and gluing.\n",
      "\n",
      "**The Meshing Module:** This module lets you create, refine, and validate meshes. It also lets you manage parallel computing, and perform a mesh-based analysis. You can perform all these operations on the geometrical models that you’ve created, or on the geometric models that other modules provide. You can also access the results of the study of the physical models (FEA and CFD) and apply them to your geometrical models by combining (coupling) the 2 analyses.\n",
      "\n",
      "**The PMI Module:** This module lets you define product model metadata that correspond to the manufacturing requirements of a product, such as quality and quantity.\n",
      "\n",
      "**The Pre-Post Module:** It allows you to pre-process, edit, and post-process data used in numerical simulation, as well as perform statistical analysis based on numerical simulation data.\n",
      "\n",
      "**The Numerical Modeling Module: 2.0.6** It lets you solve physical problems related to design performance.\n",
      "\n",
      "This lets you perform analyses such as structural analyses, fluid mechanical analyses, heat transfer analyses, electromagnetical analyses, and electrostatics analyses.\n",
      "\n",
      "**The CAD-driven Module: 2.0.5** It also allows your components to be positioned automatically.\n",
      "\n",
      "**The Advanced Data Exchange Module:** It lets you export data to SALOME using ODBC and SQL.\n",
      "\n",
      "The module includes two programs (SALOMIA), which store information on COSMO, and allow you to create a database of COSMO results (SALOME ADE) or to link to the database provided by another program (SALOME ADE).\n",
      "\n",
      "**The CFD Module:** It also lets you import files of other types of format (not only the SALOME File), such as STL, STEP, VDA FEA, or CGR formats. This is useful when importing data from other platforms and performing subsequent numerical simulations on these data.\n",
      "\n",
      "**The Pre-processor Module:** Its task is to import and export different data in a wide range of numerical formats (such as PWST, VDA FEA, etc.).\n",
      "\n",
      "**CST Studio:** It is a commercial product from Dassault Systèmes CATIA, a tool that lets you perform electromagnetical analyses. The CST Studio module allows your CST Studio files to be exported directly to SALOME using CST STUDIO.\n",
      "\n",
      "**The CCFD Package:** It lets you import a large amount of SALOME data and convert the file to a CFD-optimized format (.ccfd), with pre-defined parameters:\n",
      "- A specific solver, defined by the analysis manager;\n",
      "- Automatic creation of 3D mesh with specific solver rules.\n",
      "\n",
      "You can, optionally, create specific 3D mesh with the rules given for a certain solver.\n",
      "\n",
      "\n",
      "\n",
      "That is not all, but I think I'll bore your ears, I hope we'll have the chance to chat again!</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Réponse prédite par le modèle\n",
    "toto = generate_response(prompt1, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05dc1d06-0899-445a-a4f1-f97d96be3116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization du prompt:\n",
      "{'input_ids': tensor([[    1,  1542,  3901,  3880, 29515,   781,  9311,  1040,  4625,  3555,\n",
      "          1066,  2999,  1032,  3667, 29491,   781,   781, 28100, 12000, 29515,\n",
      "           781,  3963,  1117,  1040,  1102, 19540,  8750,  6832,  5168,  2318,\n",
      "           781,   781, 28100, 12875, 29515]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "\n",
      "Affichage des tokens:\n",
      "['<s>', '▁###', '▁Inst', 'ruction', ':', '<0x0A>', 'Use', '▁the', '▁provided', '▁input', '▁to', '▁create', '▁a', '▁response', '.', '<0x0A>', '<0x0A>', '###', '▁Input', ':', '<0x0A>', 'What', '▁is', '▁the', '▁C', 'EA', '▁Research', '▁Center', '▁website', '▁?', '<0x0A>', '<0x0A>', '###', '▁Response', ':']\n",
      "\n",
      "\n",
      "Chargement dans cuda:\n",
      "{'input_ids': tensor([[    1,  1542,  3901,  3880, 29515,   781,  9311,  1040,  4625,  3555,\n",
      "          1066,  2999,  1032,  3667, 29491,   781,   781, 28100, 12000, 29515,\n",
      "           781,  3963,  1117,  1040,  1102, 19540,  8750,  6832,  5168,  2318,\n",
      "           781,   781, 28100, 12875, 29515]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "\n",
      "IDs générés:\n",
      "tensor([[    1,  1542,  3901,  3880, 29515,   781,  9311,  1040,  4625,  3555,\n",
      "          1066,  2999,  1032,  3667, 29491,   781,   781, 28100, 12000, 29515,\n",
      "           781,  3963,  1117,  1040,  1102, 19540,  8750,  6832,  5168,  2318,\n",
      "           781,   781, 28100, 12875, 29515,   781, 29509,  6739,  1512,  1070,\n",
      "          1040,  1102, 19540,  2839, 29577, 29481,  4100,  1117,  1066,  4000,\n",
      "          1124,  1040,  6860,  1070,  4719, 15541, 23859,  1124,  3698,  6958,\n",
      "          1827,  1040, 20410,  1070,  3076,  2713, 19746, 29515,  3698, 29501,\n",
      "         12509, 29493,  3698,  1065, 16875, 29493,  3698,  1122, 16875,  1072,\n",
      "          3698,  1066, 16875, 29491,   781,   781, 28100, 12875,  5617,  2217,\n",
      "         29515,   781, 11766,  3013,  1482,  3698,  1066, 11984,   781,   781,\n",
      "         28100,  1167, 29149, 29515,   781,  1782,  1102, 19540,  8750,  6832,\n",
      "          1117, 29466,  4100,  1124,  4719, 15541, 23859,  1093, 12509, 29499,\n",
      "          1072,  1639,  5396,  5856,  1124,  3698,  6958,  1065,  3720, 29491,\n",
      "          1429, 11342,  5772,  1137, 16875,  1390,  1115,  1032,  3782,  3466,\n",
      "          1070,  1639, 16823, 21623,  1072,  1137,  1146,  1117,  3046,  1065,\n",
      "          1040,  4294,  6709,  1072, 16823, 11243, 29491,   781,   781,  1832,\n",
      "          6067,   781, 14708, 29600,   781, 29508,  1071,  3667,  1482, 29473,\n",
      "          3698,  1066, 11984,   781, 29518,  1060,  3667,  1482, 29473,  3698,\n",
      "          1066, 11984,   781, 29538,  6008,  3667,  1482, 29473,  3698,  1066,\n",
      "         11984,   781,   781, 14708, 29600,     2]], device='cuda:0')\n",
      "\n",
      "\n",
      "Décodage des IDs générés:\n",
      "<s> ### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "What is the CEA Research Center website ?\n",
      "\n",
      "### Response:\n",
      "A central part of the CEA group’s research is to focus on the effects of Artificial Intelligence on human society through the lens of four key themes: human-AI, human in AI, human for AI and human to AI.\n",
      "\n",
      "### Response Formats:\n",
      "Free text : human to ai\n",
      "\n",
      "### Rationale:\n",
      "The CEA Research Center is conducting research on Artificial Intelligence (AI) and its potential impact on human society in general. It emphasizes that AI will be a major area of its strategic investments and that it is important in the global race and strategic choices.\n",
      "\n",
      "## Other\n",
      "```\n",
      "1st response :  human to ai\n",
      "2nd response :  human to ai\n",
      "3rd response :  human to ai\n",
      "\n",
      "```</s>\n",
      "\n",
      "\n",
      "Suppression du prompt:\n",
      "<s> \n",
      "A central part of the CEA group’s research is to focus on the effects of Artificial Intelligence on human society through the lens of four key themes: human-AI, human in AI, human for AI and human to AI.\n",
      "\n",
      "### Response Formats:\n",
      "Free text : human to ai\n",
      "\n",
      "### Rationale:\n",
      "The CEA Research Center is conducting research on Artificial Intelligence (AI) and its potential impact on human society in general. It emphasizes that AI will be a major area of its strategic investments and that it is important in the global race and strategic choices.\n",
      "\n",
      "## Other\n",
      "```\n",
      "1st response :  human to ai\n",
      "2nd response :  human to ai\n",
      "3rd response :  human to ai\n",
      "\n",
      "```</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Réponse prédite par le modèle\n",
    "toto = generate_response(prompt2, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72991509-725d-4093-adea-3b5333009074",
   "metadata": {},
   "source": [
    "## Sauvegarde du modèle sur le disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebdccb16-52da-4927-adf4-84f952865fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/mistral7b_tokenizer_not_fine-tune/tokenizer_config.json',\n",
       " '../models/mistral7b_tokenizer_not_fine-tune/special_tokens_map.json',\n",
       " '../models/mistral7b_tokenizer_not_fine-tune/tokenizer.model',\n",
       " '../models/mistral7b_tokenizer_not_fine-tune/added_tokens.json',\n",
       " '../models/mistral7b_tokenizer_not_fine-tune/tokenizer.json')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spécification du répertoire de sauvegarde\n",
    "save_directory_model = \"../models/mistral7b_not_fine-tune\"\n",
    "save_directory_tokenizer = \"../models/mistral7b_tokenizer_not_fine-tune\"\n",
    "\n",
    "# Sauvegarde du modèle non fine-tuné sur le disque\n",
    "model.save_pretrained(save_directory_model)\n",
    "\n",
    "# Sauvegarde du tokenizer non fine-tuné sur le disque\n",
    "tokenizer.save_pretrained(save_directory_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage-cea-chatbot",
   "language": "python",
   "name": "stage-cea-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
