#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().system(' pip install transformers trl accelerate torch bitsandbytes peft datasets -qU')


# In[13]:


from datasets import load_dataset

instruct_tune_dataset = load_dataset("mosaicml/instruct-v3")


# In[3]:


get_ipython().system('pip install huggingface_hub')


# In[4]:


from huggingface_hub import notebook_login


# In[5]:


notebook_login()


# In[6]:


instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x["source"] == "dolly_hhrlhf")
instruct_tune_dataset


# In[7]:


instruct_tune_dataset["train"] = instruct_tune_dataset["train"].select(range(10))
instruct_tune_dataset["test"] = instruct_tune_dataset["test"].select(range(5))
instruct_tune_dataset


# In[8]:


def create_prompt(sample):
  bos_token = "<s>"
  original_system_message = "Below is an instruction that describes a task. Write a response that appropriately completes the request."
  system_message = "Use the provided input to create an instruction that could have been used to generate the response with an LLM."
  response = sample["prompt"].replace(original_system_message, "").replace("\n\n### Instruction\n", "").replace("\n### Response\n", "").strip()
  input = sample["response"]
  eos_token = "</s>"

  full_prompt = ""
  full_prompt += bos_token
  full_prompt += "### Instruction:"
  full_prompt += "\n" + system_message
  full_prompt += "\n\n### Input:"
  full_prompt += "\n" + input
  full_prompt += "\n\n### Response:"
  full_prompt += "\n" + response
  full_prompt += eos_token

  return full_prompt


# In[9]:


print(create_prompt(instruct_tune_dataset["train"][0]))


# In[10]:


print(create_prompt(instruct_tune_dataset["train"][7]))


# In[11]:


from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

nf4_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_use_double_quant=True,
   bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    device_map='auto',
    quantization_config=nf4_config,
    use_cache=False
)

tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# In[ ]:





# In[ ]:





# In[ ]:


def generate_response(prompt, model):
  encoded_input = tokenizer(prompt,  return_tensors="pt", add_special_tokens=True)
  model_inputs = encoded_input.to('cuda')

  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)

  decoded_output = tokenizer.batch_decode(generated_ids)

  return decoded_output[0].replace(prompt, "")


# In[ ]:


generate_response("### Instruction:\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\n\n### Input:\nI think it depends a little on the individual, but there are a number of steps you’ll need to take.  First, you’ll need to get a college education.  This might include a four-year undergraduate degree and a four-year doctorate program.  You’ll also need to complete a residency program.  Once you have your education, you’ll need to be licensed.  And finally, you’ll need to establish a practice.\n\n### Response:", model)


# In[ ]:


generate_response("### Instruction:\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\n\n### Input:\nI think it depends a little on the individual, but there are a number of steps you’ll need to take.  First, you’ll need to get a college education.  This might include a four-year undergraduate degree and a four-year doctorate program.  You’ll also need to complete a residency program.  Once you have your education, you’ll need to be licensed.  And finally, you’ll need to establish a practice.\n\n### Response:", model)


# In[ ]:


from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias="none",
    task_type="CAUSAL_LM"
)


# In[ ]:


model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)


# In[ ]:


from transformers import TrainingArguments

args = TrainingArguments(
  output_dir = "mistral_instruct_generation",
  #num_train_epochs=5,
  max_steps = 10, # comment out this line if you want to train in epochs
  per_device_train_batch_size = 1,
  warmup_steps = 0,
  logging_steps=2,
  save_strategy="epoch",
  #evaluation_strategy="epoch",
  evaluation_strategy="steps",
  eval_steps=2, # comment out this line if you want to evaluate at the end of each epoch
  learning_rate=2e-4,
  bf16=True,
  lr_scheduler_type='constant',
)


# In[ ]:


from trl import SFTTrainer

max_seq_length = 256

trainer = SFTTrainer(
  model=model,
  peft_config=peft_config,
  max_seq_length=max_seq_length,
  tokenizer=tokenizer,
  packing=True,
  formatting_func=create_prompt,
  args=args,
  train_dataset=instruct_tune_dataset["train"],
  eval_dataset=instruct_tune_dataset["test"]
)


# In[ ]:


trainer.train()


# In[ ]:


new_model = "Mistral-7b-v2-finetune-TEST"


# In[ ]:


trainer.model.save_pretrained(new_model)


# In[ ]:


import os; os.getcwd()



# In[ ]:


get_ipython().system('pip install wandb')


# In[ ]:


import os, torch, wandb


# In[ ]:


model.config.use_cache = True


# In[ ]:


model.eval()


# In[ ]:


from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging

prompt = "Can I find information about the code's approach to handling long-running tasks and background jobs?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=50)
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])


# In[ ]:





# In[ ]:


prompt = "Can I find information about SALOME?"


# In[ ]:


result = pipe(f"<s>[INST] {prompt} [/INST]")


# In[ ]:





# In[ ]:


print(result[0]['generated_text'])


# In[ ]:


nf4_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_use_double_quant=True,
   bnb_4bit_compute_dtype=torch.bfloat16
)

modelb = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    device_map='auto',
    quantization_config=nf4_config,
    use_cache=False
)

tokenizerb = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

tokenizerb.pad_token = tokenizer.eos_token
tokenizerb.padding_side = "right"


# In[ ]:


pipeb = pipeline(task="text-generation", model=modelb, tokenizer=tokenizerb, max_length=400)


# In[ ]:





# In[ ]:


prompt = "Can I find information about SALOME?"
result = pipeb(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])


# In[ ]:





# In[ ]:


result[0]


# In[ ]:


result


# In[ ]:


prompt = ["<s>[INST]Can I find information about SALOME?[INST]", "<s>[INST]Can I find information about CEA?[INST]"]


# In[ ]:


result = pipeb(prompt)


# In[ ]:


result

