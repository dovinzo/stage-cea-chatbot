{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c482fe3-5884-43a6-a2fa-2c3f2a6502a6",
   "metadata": {},
   "source": [
    "# Fine-Tuning de Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1d0f7-d132-4e6e-ae2e-b9837c3b1e95",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de5505-866b-419d-93c8-d2dd36cfc11d",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons effectuer le fine-tuning de Mistral-7B (https://huggingface.co/mistralai/Mistral-7B-v0.3) à partir de l'API Transformers d'Hugging Face (utilisant LoRa). Les étapes suivantes seront suivies :\n",
    "\n",
    "1. Installation des packages.\n",
    "2. Importation des bibliothèques nécessaires.\n",
    "3. Récupération des données.\n",
    "4. Prétraitement des données.\n",
    "5. Récupération du modèle sur le disque.\n",
    "6. Entraînement du modèle à partir des données.\n",
    "7. Utilisation du modèle entraîné sur un cas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46ba7f-ec69-46a4-8509-7a641ab8a9da",
   "metadata": {},
   "source": [
    "## Installation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bd8cee-c1ee-41c6-8425-0ede6a975da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers[sentencepiece] trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2153b6e9-89e6-481f-9a5e-4018a1ef4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/product/ubuntu22-x86_64/apps/CUDA/12.1.0/nvvm/lib64:/product/ubuntu22-x86_64/apps/CUDA/12.1.0/extras/CUPTI/lib64:/product/ubuntu22-x86_64/apps/CUDA/12.1.0/lib\n"
     ]
    }
   ],
   "source": [
    "import os; print(os.getenv(\"LD_LIBRARY_PATH\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b82e5-4fcc-4ec9-9e58-569344b18274",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94f7234-1915-46ee-9102-9fa6606db872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour télécharger les données\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Pour avoir l'accès au Hub d'Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Pour récupérer Mistral-7B, le tokenizer associé et ...\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Pour utiliser PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f6e74-a06e-4c7f-a18e-a49344ed79c3",
   "metadata": {},
   "source": [
    "## Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abcbd8af-a539-4ad5-9553-6d9ed4e8e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af8e51a-861f-4a5f-b2ca-e361bafe95e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 56167\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 6807\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b24eb-a768-40fb-92d6-5aa7f036a58d",
   "metadata": {},
   "source": [
    "## Prétraitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a108d5e-eff2-4f33-8060-50ec666821b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2442ea-6cfc-43df-a455-0bcdd71bb9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 34333\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 4771\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36cd401b-22c3-44e0-8fdb-1215e08a1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000)) # TODO : Rajouter la méthode shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c47a5b-0378-45eb-ad46-5e40c7eb7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200)) # TODO : Rajouter la méthode shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80739a48-a6ed-4be3-a8e0-e0cafceee09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cde1c04-ed62-4d88-af5c-ba221494efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction\\nWhat are different types of grass?\\n\\n### Response\\n', 'response': 'There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.', 'source': 'dolly_hhrlhf'}\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'une donnée (ici une donnée d'entraînement)\n",
    "print(instruct_tune_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03e1ef0-12f4-44a6-874d-3656e9378364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction\n",
      "What are different types of grass?\n",
      "\n",
      "### Response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple de prompt d'une donnée originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2879ed18-790d-4be7-88d3-72bf068e6a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n"
     ]
    }
   ],
   "source": [
    "# Exemple de response d'une donnée originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f35996-5b5e-44d3-89f0-a797aa3108ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dolly_hhrlhf\n"
     ]
    }
   ],
   "source": [
    "# Exemple de source d'une donnée originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86791009-bf09-4a9f-8ae1-a5fc5b284ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    \"\"\"\n",
    "    Modifie la donnée d'entrée pour correspondre au format attendu par Mistral-7B et à la tâche en question.\n",
    "\n",
    "    Paramètres:\n",
    "    sample (dict): La donnée d'entrée.\n",
    "\n",
    "    Retours:\n",
    "    str: La modification sous le bon format attendu par Mistral-7B et à la tâche en question.\n",
    "\n",
    "    Exemple:\n",
    "    >>> print(create_prompt({'prompt': \"Can I find information about SALOME platform ?\", 'response': \"\"}))\n",
    "    \n",
    "    <s>### Instruction:\n",
    "    Use the provided input to create a response to the prompt question.\n",
    "    \n",
    "    ### Input:\n",
    "    Can I find information about SALOME platform ?\n",
    "    \n",
    "    ### Response:\n",
    "    </s>\n",
    "    \"\"\"\n",
    "    \n",
    "    bos_token = \"<s>\"\n",
    "    original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    system_message = \"Use the provided input to create a response to the prompt question.\"\n",
    "    response = sample[\"response\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "    input = sample[\"prompt\"]\n",
    "    eos_token = \"</s>\"\n",
    "    \n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += \"### Instruction:\"\n",
    "    full_prompt += \"\\n\" + system_message\n",
    "    full_prompt += \"\\n\\n### Input:\"\n",
    "    full_prompt += \"\\n\" + input\n",
    "    full_prompt += \"\\n\\n### Response:\"\n",
    "    full_prompt += \"\\n\" + response\n",
    "    full_prompt += eos_token\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a11856fb-024f-40ae-aeb2-23fa6c33dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Modifie la donnée d'entrée pour correspondre au format attendu par Mistral-7B et à la tâche en question.\n",
      "\n",
      "    Paramètres:\n",
      "    sample (dict): La donnée d'entrée.\n",
      "\n",
      "    Retours:\n",
      "    str: La modification sous le bon format attendu par Mistral-7B et à la tâche en question.\n",
      "\n",
      "    Exemple:\n",
      "    >>> print(create_prompt({'prompt': \"Can I find information about SALOME platform ?\", 'response': \"\"}))\n",
      "    \n",
      "    <s>### Instruction:\n",
      "    Use the provided input to create a response to the prompt question.\n",
      "    \n",
      "    ### Input:\n",
      "    Can I find information about SALOME platform ?\n",
      "    \n",
      "    ### Response:\n",
      "    </s>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Affichage de la doc de la fonction create_prompt\n",
    "print(create_prompt.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e4d6219-debe-44e2-92d7-a6ac58ca8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "Use the provided input to create a response to the prompt question.\n",
      "\n",
      "### Input:\n",
      "Can I find information about SALOME platform ?\n",
      "\n",
      "### Response:\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation de la fonction create_prompt\n",
    "print(create_prompt({'prompt': \"Can I find information about SALOME platform ?\", 'response': \"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb331ad-8f73-441b-a1a4-493b34264c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "Use the provided input to create a response to the prompt question.\n",
      "\n",
      "### Input:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction\n",
      "What are different types of grass?\n",
      "\n",
      "### Response\n",
      "\n",
      "\n",
      "### Response:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.</s>\n"
     ]
    }
   ],
   "source": [
    "# Exemple de la donnée finale, i.e. après passage dans la fonction create_prompt\n",
    "print(create_prompt(instruct_tune_dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8c108-9f5f-4ec7-9992-71cb1bb7369e",
   "metadata": {},
   "source": [
    "## Récupération du modèle Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36f146-52af-4564-be67-0a2a6879dffb",
   "metadata": {},
   "source": [
    "Dans cette section, on télécharge/récupère le modèle de base Mistral-7B, se trouvant sur le disque, et le tokenizer associé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd619abd-7725-4c45-a638-4823760fab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: À étudier\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b693b1c-db2a-4f10-8321-93bebbe0a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécification du répertoire de sauvegarde du modèle\n",
    "save_directory_model = \"../models/mistral7b_not_fine-tune\"\n",
    "\n",
    "# Spécification du répertoire de sauvegarde du tokenizer\n",
    "save_directory_tokenizer = \"../models/mistral7b_tokenizer_not_fine-tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9834b11c-51f4-45e0-a3e4-d30e8d9f9f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "# Chargement du modèle sauvegardé\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_directory_model,\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f12f1e95-c2ad-4fa8-bf62-ee466d5acc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du tokenizer associé\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory_tokenizer)\n",
    "\n",
    "# Problème: ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.\n",
    "# Solution: pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34fa0fdb-0d47-412e-97f6-151b964d5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: À étudier\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847286a-5d6a-476c-a68d-0db7bd94e9e7",
   "metadata": {},
   "source": [
    "## Fine-tuning du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9387cf21-b0cf-4542-880a-c9dbaa390902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d492a4-49cb-400c-ac43-008484c1ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "235a59ab-cb90-443d-99cb-ef58259704da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"mistral_instruct_generation\",\n",
    "    #num_train_epochs=5,00:02.0\n",
    "    max_steps =5, # comment out this line if you want to train in epochs\n",
    "    per_device_train_batch_size = 2,\n",
    "    warmup_steps = 0,\n",
    "    logging_steps=1,\n",
    "    #save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=2, # comment out this line if you want to evaluate at the end of each epoch\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    per_gpu_train_batch_size=1,\n",
    "    lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b309a2d7-572e-4fb5-8f58-2d3eb641a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:421: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 128\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt,\n",
    "  args=args,\n",
    "  train_dataset=instruct_tune_dataset[\"train\"],\n",
    "  eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36899bd8-4e2d-4d45-ad3c-25b0bdfe1022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:440\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 440\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1959\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1961\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   1963\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/trainer.py:904\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    901\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n\u001b[1;32m    902\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefetch_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_prefetch_factor\n\u001b[0;32m--> 904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataloader_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/accelerate/accelerator.py:1255\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(obj)\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mNO\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_BYPASS_DEVICE_MAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1249\u001b[0m     ):\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded with `device_map=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` in any distributed mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mmyscript.py}}`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1253\u001b[0m         )\n\u001b[0;32m-> 1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1256\u001b[0m     model_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m args:\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/accelerate/accelerator.py:527\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/accelerate/state.py:1073\u001b[0m, in \u001b[0;36mAcceleratorState.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;66;03m# By this point we know that no attributes of `self` contain `name`,\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;66;03m# so we just modify the error message\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_attrs:\n\u001b[0;32m-> 1073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`AcceleratorState` object has no attribute `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis happens if `AcceleratorState._reset_state()` was called and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1076\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man `Accelerator` or `PartialState` was not reinitialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1077\u001b[0m         )\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# Raise a typical AttributeError\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcceleratorState\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a202f3-b299-4b5f-a7f9-5a74296014c9",
   "metadata": {},
   "source": [
    "## Prédiction à partir du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eff223d-232a-4bea-b9b7-4a69fb075d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1c291-f5a4-4a32-8c21-1dea29a5503b",
   "metadata": {},
   "source": [
    "Exemple d'utilisation de la fonction `generate_response` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eedd47e0-3c9a-4ab5-ac47-aa852152fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "Can I find information about SALOME platform ?\n",
      "\n",
      "### Response:</s>\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "#prompt = \"<s>### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nI think it depends a little on the individual, but there are a number of steps you’ll need to take.  First, you’ll need to get a college education.  This might include a four-year undergraduate degree and a four-year doctorate program.  You’ll also need to complete a residency program.  Once you have your education, you’ll need to be licensed.  And finally, you’ll need to establish a practice.\\n\\n### Response:\"\n",
    "#prompt = create_prompt()\n",
    "prompt = \"<s>### Instruction:\\nUse the provided input to create a response.\\n\\n### Input:\\nCan I find information about SALOME platform ?\\n\\n### Response:</s>\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0abde696-8e5b-44ce-8b0a-c3bb4bf4948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>-\n",
      "\n",
      "### Output:\n",
      "\n",
      "Dear *NAME*,\n",
      "\n",
      "Thank you, as a result of our conversation, the *YOUR NAME* user has been registered on the SALOME platform in \"Account verification\" status.\n",
      "\n",
      "Please go to *YOUR SALOME LINK* to confirm your e-mail address and login to platform.\n",
      "\n",
      "###### Information\n",
      "You have activated your account with success. Click on the button to start your journey from Open Science.\n",
      "\n",
      "###### Regards,\n",
      "\n",
      "SALOME team, SOLABS Co, LLC</s>\n"
     ]
    }
   ],
   "source": [
    "# Réponse prédite par le modèle\n",
    "print(generate_response(prompt, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72991509-725d-4093-adea-3b5333009074",
   "metadata": {},
   "source": [
    "## Sauvegarde du modèle sur le disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebdccb16-52da-4927-adf4-84f952865fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spécification du répertoire de sauvegarde\n",
    "save_directory = \"../models/mistral7b_fine-tune\"\n",
    "\n",
    "# Sauvegarde du modèle fine-tuné\n",
    "model.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage-cea-chatbot",
   "language": "python",
   "name": "stage-cea-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
