{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c482fe3-5884-43a6-a2fa-2c3f2a6502a6",
   "metadata": {},
   "source": [
    "# Fine-Tuning de Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1d0f7-d132-4e6e-ae2e-b9837c3b1e95",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de5505-866b-419d-93c8-d2dd36cfc11d",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons effectuer le fine-tuning de Mistral-7B (https://huggingface.co/mistralai/Mistral-7B-v0.3) √† partir de l'API Transformers d'Hugging Face (utilisant LoRa). Les √©tapes suivantes seront suivies :\n",
    "\n",
    "1. Installation des packages.\n",
    "2. Importation des biblioth√®ques n√©cessaires.\n",
    "3. R√©cup√©ration des donn√©es.\n",
    "4. Pr√©traitement des donn√©es.\n",
    "5. R√©cup√©ration du mod√®le sur le disque.\n",
    "6. Entra√Ænement du mod√®le √† partir des donn√©es.\n",
    "7. Utilisation du mod√®le entra√Æn√© sur un cas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46ba7f-ec69-46a4-8509-7a641ab8a9da",
   "metadata": {},
   "source": [
    "## Installation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09bd8cee-c1ee-41c6-8425-0ede6a975da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers[sentencepiece] trl accelerate torch bitsandbytes peft datasets -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2153b6e9-89e6-481f-9a5e-4018a1ef4630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/product/ubuntu22-x86_64/apps/CUDA/12.1.0/nvvm/lib64:/product/ubuntu22-x86_64/apps/CUDA/12.1.0/extras/CUPTI/lib64:/product/ubuntu22-x86_64/apps/CUDA/12.1.0/lib\n"
     ]
    }
   ],
   "source": [
    "import os; print(os.getenv(\"LD_LIBRARY_PATH\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b82e5-4fcc-4ec9-9e58-569344b18274",
   "metadata": {},
   "source": [
    "## Importation des biblioth√®ques n√©cessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94f7234-1915-46ee-9102-9fa6606db872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour t√©l√©charger les donn√©es\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Pour avoir l'acc√®s au Hub d'Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Pour r√©cup√©rer Mistral-7B, le tokenizer associ√© et ...\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Pour utiliser PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20f6e74-a06e-4c7f-a18e-a49344ed79c3",
   "metadata": {},
   "source": [
    "## R√©cup√©ration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abcbd8af-a539-4ad5-9553-6d9ed4e8e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset = load_dataset(\"mosaicml/instruct-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af8e51a-861f-4a5f-b2ca-e361bafe95e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 56167\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 6807\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b24eb-a768-40fb-92d6-5aa7f036a58d",
   "metadata": {},
   "source": [
    "## Pr√©traitement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a108d5e-eff2-4f33-8060-50ec666821b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset = instruct_tune_dataset.filter(lambda x: x[\"source\"] == \"dolly_hhrlhf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2442ea-6cfc-43df-a455-0bcdd71bb9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 34333\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 4771\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36cd401b-22c3-44e0-8fdb-1215e08a1128",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset[\"train\"] = instruct_tune_dataset[\"train\"].select(range(5_000)) # TODO : Rajouter la m√©thode shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c47a5b-0378-45eb-ad46-5e40c7eb7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_tune_dataset[\"test\"] = instruct_tune_dataset[\"test\"].select(range(200)) # TODO : Rajouter la m√©thode shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80739a48-a6ed-4be3-a8e0-e0cafceee09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'response', 'source'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(instruct_tune_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cde1c04-ed62-4d88-af5c-ba221494efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction\\nWhat are different types of grass?\\n\\n### Response\\n', 'response': 'There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.', 'source': 'dolly_hhrlhf'}\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'une donn√©e (ici une donn√©e d'entra√Ænement)\n",
    "print(instruct_tune_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03e1ef0-12f4-44a6-874d-3656e9378364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction\n",
      "What are different types of grass?\n",
      "\n",
      "### Response\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemple de prompt d'une donn√©e originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2879ed18-790d-4be7-88d3-72bf068e6a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.\n"
     ]
    }
   ],
   "source": [
    "# Exemple de response d'une donn√©e originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f35996-5b5e-44d3-89f0-a797aa3108ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dolly_hhrlhf\n"
     ]
    }
   ],
   "source": [
    "# Exemple de source d'une donn√©e originale\n",
    "print(instruct_tune_dataset[\"train\"][0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86791009-bf09-4a9f-8ae1-a5fc5b284ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    \"\"\"\n",
    "    Modifie la donn√©e d'entr√©e pour correspondre au format attendu par Mistral-7B et √† la t√¢che en question.\n",
    "\n",
    "    Param√®tres:\n",
    "    sample (dict): La donn√©e d'entr√©e.\n",
    "\n",
    "    Retours:\n",
    "    str: La modification sous le bon format attendu par Mistral-7B et √† la t√¢che en question.\n",
    "\n",
    "    Exemple:\n",
    "    >>> print(create_prompt({'prompt': \"Can I find information about SALOME platform ?\", 'response': \"\"}))\n",
    "    \n",
    "    <s>### Instruction:\n",
    "    Use the provided input to create a response to the prompt question.\n",
    "    \n",
    "    ### Input:\n",
    "    Can I find information about SALOME platform ?\n",
    "    \n",
    "    ### Response:\n",
    "    </s>\n",
    "    \"\"\"\n",
    "    \n",
    "    bos_token = \"<s>\"\n",
    "    original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    system_message = \"Use the provided input to create a response to the prompt question.\"\n",
    "    response = sample[\"response\"].replace(original_system_message, \"\").replace(\"\\n\\n### Instruction\\n\", \"\").replace(\"\\n### Response\\n\", \"\").strip()\n",
    "    input = sample[\"prompt\"]\n",
    "    eos_token = \"</s>\"\n",
    "    \n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += \"### Instruction:\"\n",
    "    full_prompt += \"\\n\" + system_message\n",
    "    full_prompt += \"\\n\\n### Input:\"\n",
    "    full_prompt += \"\\n\" + input\n",
    "    full_prompt += \"\\n\\n### Response:\"\n",
    "    full_prompt += \"\\n\" + response\n",
    "    full_prompt += eos_token\n",
    "    \n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a11856fb-024f-40ae-aeb2-23fa6c33dda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Modifie la donn√©e d'entr√©e pour correspondre au format attendu par Mistral-7B et √† la t√¢che en question.\n",
      "\n",
      "    Param√®tres:\n",
      "    sample (dict): La donn√©e d'entr√©e.\n",
      "\n",
      "    Retours:\n",
      "    str: La modification sous le bon format attendu par Mistral-7B et √† la t√¢che en question.\n",
      "\n",
      "    Exemple:\n",
      "    >>> print(create_prompt({'prompt': \"Can I find information about SALOME platform ?\", 'response': \"\"}))\n",
      "    \n",
      "    <s>### Instruction:\n",
      "    Use the provided input to create a response to the prompt question.\n",
      "    \n",
      "    ### Input:\n",
      "    Can I find information about SALOME platform ?\n",
      "    \n",
      "    ### Response:\n",
      "    </s>\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Affichage de la doc de la fonction create_prompt\n",
    "print(create_prompt.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e4d6219-debe-44e2-92d7-a6ac58ca8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "Use the provided input to create a response to the prompt question.\n",
      "\n",
      "### Input:\n",
      "Can I find information about SALOME platform ?\n",
      "\n",
      "### Response:\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation de la fonction create_prompt\n",
    "print(create_prompt({'prompt': \"Can I find information about SALOME platform ?\", 'response': \"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb331ad-8f73-441b-a1a4-493b34264c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "Use the provided input to create a response to the prompt question.\n",
      "\n",
      "### Input:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction\n",
      "What are different types of grass?\n",
      "\n",
      "### Response\n",
      "\n",
      "\n",
      "### Response:\n",
      "There are more than 12,000 species of grass. The most common is Kentucky Bluegrass, because it grows quickly, easily, and is soft to the touch. Rygrass is shiny and bright green colored. Fescues are dark green and shiny. Bermuda grass is harder but can grow in drier soil.</s>\n"
     ]
    }
   ],
   "source": [
    "# Exemple de la donn√©e finale, i.e. apr√®s passage dans la fonction create_prompt\n",
    "print(create_prompt(instruct_tune_dataset[\"train\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8c108-9f5f-4ec7-9992-71cb1bb7369e",
   "metadata": {},
   "source": [
    "## R√©cup√©ration du mod√®le Mistral-7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36f146-52af-4564-be67-0a2a6879dffb",
   "metadata": {},
   "source": [
    "Dans cette section, on t√©l√©charge/r√©cup√®re le mod√®le de base Mistral-7B, se trouvant sur le disque, et le tokenizer associ√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd619abd-7725-4c45-a638-4823760fab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: √Ä √©tudier\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b693b1c-db2a-4f10-8321-93bebbe0a579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sp√©cification du r√©pertoire de sauvegarde du mod√®le\n",
    "save_directory_model = \"../models/mistral7b_not_fine-tune\"\n",
    "\n",
    "# Sp√©cification du r√©pertoire de sauvegarde du tokenizer\n",
    "save_directory_tokenizer = \"../models/mistral7b_tokenizer_not_fine-tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9834b11c-51f4-45e0-a3e4-d30e8d9f9f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "# Chargement du mod√®le sauvegard√©\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_directory_model,\n",
    "    device_map='auto',\n",
    "    quantization_config=nf4_config,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f12f1e95-c2ad-4fa8-bf62-ee466d5acc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©ration du tokenizer associ√©\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory_tokenizer)\n",
    "\n",
    "# Probl√®me: ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.\n",
    "# Solution: pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34fa0fdb-0d47-412e-97f6-151b964d5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: √Ä √©tudier\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847286a-5d6a-476c-a68d-0db7bd94e9e7",
   "metadata": {},
   "source": [
    "## Fine-tuning du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9387cf21-b0cf-4542-880a-c9dbaa390902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d492a4-49cb-400c-ac43-008484c1ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "235a59ab-cb90-443d-99cb-ef58259704da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"mistral_instruct_generation\",\n",
    "    #num_train_epochs=5,00:02.0\n",
    "    max_steps =5, # comment out this line if you want to train in epochs\n",
    "    per_device_train_batch_size = 2,\n",
    "    warmup_steps = 0,\n",
    "    logging_steps=1,\n",
    "    #save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=2, # comment out this line if you want to evaluate at the end of each epoch\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    per_gpu_train_batch_size=1,\n",
    "    lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b309a2d7-572e-4fb5-8f58-2d3eb641a9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:181: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:421: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 128\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=create_prompt,\n",
    "  args=args,\n",
    "  train_dataset=instruct_tune_dataset[\"train\"],\n",
    "  eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36899bd8-4e2d-4d45-ad3c-25b0bdfe1022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "`AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:440\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 440\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1959\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1961\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   1963\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/transformers/trainer.py:904\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    901\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_init_fn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m seed_worker\n\u001b[1;32m    902\u001b[0m     dataloader_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefetch_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdataloader_prefetch_factor\n\u001b[0;32m--> 904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataloader_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/accelerate/accelerator.py:1255\u001b[0m, in \u001b[0;36mAccelerator.prepare\u001b[0;34m(self, device_placement, *args)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_device_map(obj)\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mNO\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mACCELERATE_BYPASS_DEVICE_MAP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1249\u001b[0m     ):\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt train a model that has been loaded with `device_map=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` in any distributed mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please rerun your script specifying `--num_processes=1` or by launching with `python \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mmyscript.py}}`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1253\u001b[0m         )\n\u001b[0;32m-> 1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   1256\u001b[0m     model_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m args:\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/accelerate/accelerator.py:527\u001b[0m, in \u001b[0;36mAccelerator.distributed_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistributed_type\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed_type\u001b[49m\n",
      "File \u001b[0;32m~/envs/stage-cea-chatbot/lib/python3.10/site-packages/accelerate/state.py:1073\u001b[0m, in \u001b[0;36mAcceleratorState.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;66;03m# By this point we know that no attributes of `self` contain `name`,\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;66;03m# so we just modify the error message\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_attrs:\n\u001b[0;32m-> 1073\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1074\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`AcceleratorState` object has no attribute `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis happens if `AcceleratorState._reset_state()` was called and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1076\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man `Accelerator` or `PartialState` was not reinitialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1077\u001b[0m         )\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# Raise a typical AttributeError\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcceleratorState\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: `AcceleratorState` object has no attribute `distributed_type`. This happens if `AcceleratorState._reset_state()` was called and an `Accelerator` or `PartialState` was not reinitialized."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a202f3-b299-4b5f-a7f9-5a74296014c9",
   "metadata": {},
   "source": [
    "## Pr√©diction √† partir du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eff223d-232a-4bea-b9b7-4a69fb075d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model):\n",
    "  encoded_input = tokenizer(prompt,  return_tensors=\"pt\", add_special_tokens=True)\n",
    "  model_inputs = encoded_input.to('cuda')\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "  decoded_output = tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "  return decoded_output[0].replace(prompt, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1c291-f5a4-4a32-8c21-1dea29a5503b",
   "metadata": {},
   "source": [
    "Exemple d'utilisation de la fonction `generate_response` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eedd47e0-3c9a-4ab5-ac47-aa852152fea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>### Instruction:\n",
      "Use the provided input to create a response.\n",
      "\n",
      "### Input:\n",
      "Can I find information about SALOME platform ?\n",
      "\n",
      "### Response:</s>\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "#prompt = \"<s>### Instruction:\\nUse the provided input to create an instruction that could have been used to generate the response with an LLM.\\n\\n### Input:\\nI think it depends a little on the individual, but there are a number of steps you‚Äôll need to take.  First, you‚Äôll need to get a college education.  This might include a four-year undergraduate degree and a four-year doctorate program.  You‚Äôll also need to complete a residency program.  Once you have your education, you‚Äôll need to be licensed.  And finally, you‚Äôll need to establish a practice.\\n\\n### Response:\"\n",
    "#prompt = create_prompt()\n",
    "prompt = \"<s>### Instruction:\\nUse the provided input to create a response.\\n\\n### Input:\\nCan I find information about SALOME platform ?\\n\\n### Response:</s>\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0abde696-8e5b-44ce-8b0a-c3bb4bf4948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/catB/kl279585/envs/stage-cea-chatbot/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>-\n",
      "\n",
      "### Output:\n",
      "\n",
      "Dear *NAME*,\n",
      "\n",
      "Thank you, as a result of our conversation, the *YOUR NAME* user has been registered on the SALOME platform in \"Account verification\" status.\n",
      "\n",
      "Please go to *YOUR SALOME LINK* to confirm your e-mail address and login to platform.\n",
      "\n",
      "###### Information\n",
      "You have activated your account with success. Click on the button to start your journey from Open Science.\n",
      "\n",
      "###### Regards,\n",
      "\n",
      "SALOME team, SOLABS Co, LLC</s>\n"
     ]
    }
   ],
   "source": [
    "# R√©ponse pr√©dite par le mod√®le\n",
    "print(generate_response(prompt, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72991509-725d-4093-adea-3b5333009074",
   "metadata": {},
   "source": [
    "## Sauvegarde du mod√®le sur le disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebdccb16-52da-4927-adf4-84f952865fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sp√©cification du r√©pertoire de sauvegarde\n",
    "save_directory = \"../models/mistral7b_fine-tune\"\n",
    "\n",
    "# Sauvegarde du mod√®le fine-tun√©\n",
    "model.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage-cea-chatbot",
   "language": "python",
   "name": "stage-cea-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
